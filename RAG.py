import streamlit as st
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch
from tqdm import tqdm

# ==========================================
# é…ç½®åŒºåŸŸï¼ˆè¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼‰
# ==========================================
ST_TITLE = "ä¸­å›½å†å² RAG é—®ç­”ç³»ç»Ÿ"

# txt æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆä¿®æ”¹ä¸ºä½ çš„å®é™…è·¯å¾„ï¼‰
TXT_FOLDER = "./data_extract/history_data"
# æœ¬åœ° Qwen2.5-7B-Instruct è·¯å¾„ï¼ˆä½ ä¹‹å‰ä¸‹è½½åˆ° pub çš„ï¼‰
# LOCAL_MODEL_PATH = "/root/autodl-tmp/qwen/Qwen2___5-7B-Instruct"
LOCAL_MODEL_PATH = "./merged_qwen_history"
# åµŒå…¥æ¨¡å‹"BAAI/bge-m3"ï¼ˆæ”¯æŒæ›´é•¿æ–‡æœ¬ï¼‰
EMBEDDING_MODEL = "BAAI/bge-m3"
# å‘é‡åº“æŒä¹…åŒ–ç›®å½•
VECTOR_DB_PATH = "./chroma_db_history"

# ==========================================
# åˆå§‹åŒ– RAG ç³»ç»Ÿï¼ˆåªè¿è¡Œä¸€æ¬¡ï¼Œç¼“å­˜ï¼‰
# ==========================================
@st.cache_resource
def initialize_rag_system():
    # 1. æ£€æŸ¥ txt æ–‡ä»¶å¤¹
    if not os.path.exists(TXT_FOLDER):
        return None, f"txt æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {TXT_FOLDER}"

    txt_files = [f for f in os.listdir(TXT_FOLDER) if f.endswith(".txt")]
    if not txt_files:
        return None, f"æ–‡ä»¶å¤¹ {TXT_FOLDER} ä¸­æ²¡æœ‰ txt æ–‡ä»¶"

    st.info(f"å‘ç° {len(txt_files)} ä¸ª txt æ–‡ä»¶ï¼Œæ­£åœ¨åŠ è½½...")

    # 2. åŠ è½½æ‰€æœ‰ txt æ–‡ä»¶
    docs = []
    for file_name in tqdm(txt_files, desc="åŠ è½½ txt æ–‡ä»¶"):
        file_path = os.path.join(TXT_FOLDER, file_name)
        loader = TextLoader(file_path, encoding="utf-8")
        docs.extend(loader.load())

    st.success(f"æˆåŠŸåŠ è½½ {len(docs)} ä¸ªæ–‡æ¡£ï¼ˆéƒ¨åˆ†å¤§æ–‡ä»¶å¯èƒ½è¢«è‡ªåŠ¨åˆ†æ®µï¼‰")

    # 3. æ–‡æœ¬åˆ‡åˆ†
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,  # é€‚åˆå†å²æ–‡æœ¬ï¼Œå¯æ ¹æ®éœ€è¦è°ƒæ•´
        chunk_overlap=100,
        length_function=len,
    )
    splits = text_splitter.split_documents(docs)
    st.info(f"åˆ‡åˆ†ä¸º {len(splits)} ä¸ª chunk")

    # 4. æœ¬åœ°åµŒå…¥æ¨¡å‹
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={'device': 'cuda'},
        encode_kwargs={'normalize_embeddings': True}
    )

    # 5. æ„å»ºæˆ–åŠ è½½å‘é‡åº“
    if os.path.exists(VECTOR_DB_PATH):
        st.info("æ£€æµ‹åˆ°å·²æœ‰å‘é‡åº“ï¼Œç›´æ¥åŠ è½½...")
        vectorstore = Chroma(persist_directory=VECTOR_DB_PATH, embedding_function=embeddings)
    else:
        st.info("æ­£åœ¨æ„å»ºå‘é‡åº“ï¼ˆé¦–æ¬¡è¿è¡Œè¾ƒæ…¢ï¼Œåç»­ç§’å¼€ï¼‰...")
        vectorstore = Chroma.from_documents(
            documents=splits,
            embedding=embeddings,
            persist_directory=VECTOR_DB_PATH
        )
        st.success("å‘é‡åº“æ„å»ºå®Œæˆå¹¶å·²ä¿å­˜ï¼")

    retriever = vectorstore.as_retriever(search_kwargs={"k": 6})

    # 6. åŠ è½½æœ¬åœ° Qwen2.5-7B-Instruct
    if not os.path.exists(LOCAL_MODEL_PATH):
        return None, f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {LOCAL_MODEL_PATH}"

    tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_PATH, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        LOCAL_MODEL_PATH,
        device_map="auto",
        torch_dtype=torch.bfloat16,
        trust_remote_code=True,
        # load_in_4bit=True,   # å¦‚æ˜¾å­˜ä¸å¤Ÿå¯å¼€å¯ï¼ˆéœ€ pip install bitsandbytesï¼‰
    )

    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=1024,
        temperature=0.3,
        top_p=0.9,
        do_sample=True,
        repetition_penalty=1.1
    )

    # æ­£ç¡®æ–¹å¼ï¼šå…ˆåŒ…è£…æˆ HuggingFacePipelineï¼Œå†ç”¨ ChatHuggingFace
    llm_pipeline = HuggingFacePipeline(pipeline=pipe)

    llm = ChatHuggingFace(
        llm=llm_pipeline,       # â† å¿…é¡»ç”¨ llm= å‚æ•°
        tokenizer=tokenizer,
        streaming=True
    )
    
    # 7. Prompt æ¨¡æ¿
    template = """
    ä½ æ˜¯ä¸€ä¸ªä¸­å›½å†å²ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼Œå‡†ç¡®ã€è¯¦å°½åœ°å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
    å¦‚æœä¸Šä¸‹æ–‡æ²¡æœ‰è¶³å¤Ÿä¿¡æ¯ï¼Œè¯·è¯´â€œæ ¹æ®å½“å‰çŸ¥è¯†åº“ï¼Œæˆ‘æ— æ³•æä¾›ç¡®åˆ‡ç­”æ¡ˆâ€ã€‚
    
    ä¸Šä¸‹æ–‡ï¼š
    {context}
    
    é—®é¢˜ï¼š{question}
    """
    prompt = ChatPromptTemplate.from_template(template)

    # 8. RAG Chain
    rag_chain = (
            {"context": retriever, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
    )

    return rag_chain, f"ç³»ç»Ÿå°±ç»ªï¼çŸ¥è¯†åº“åŒ…å« {len(txt_files)} ä¸ªå†å² txt æ–‡ä»¶"


# ==========================================
# Streamlit ç•Œé¢
# ==========================================
st.set_page_config(page_title=ST_TITLE, page_icon="ğŸ“œ")
st.title(ST_TITLE)

with st.sidebar:
    st.header("ç³»ç»ŸçŠ¶æ€")
    with st.spinner("æ­£åœ¨åˆå§‹åŒ– RAG ç³»ç»Ÿ..."):
        rag_chain, msg = initialize_rag_system()

    if rag_chain:
        st.success("âœ… RAG ç³»ç»Ÿå·²å°±ç»ª")
        st.info(msg)
        st.info(f"ğŸ§  æ¨¡å‹: æœ¬åœ° Qwen2.5-7B-Instruct\n\nğŸ“š åµŒå…¥æ¨¡å‹: {EMBEDDING_MODEL}")
    else:
        st.error(f"âŒ åˆå§‹åŒ–å¤±è´¥: {msg}")
        st.stop()

    if st.button("æ¸…é™¤å¯¹è¯å†å²"):
        st.session_state.messages = []
        st.rerun()

if "messages" not in st.session_state:
    st.session_state.messages = []

# æ˜¾ç¤ºå†å²
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("è¯·è¾“å…¥ä½ æƒ³çŸ¥é“çš„ä¸­å›½å†å²ç›¸å…³é—®é¢˜"):
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    with st.chat_message("assistant"):
        placeholder = st.empty()
        full_response = ""

        try:
            for chunk in rag_chain.stream(prompt):
                full_response += chunk
                placeholder.markdown(full_response + "â–Œ")
            placeholder.markdown(full_response)
        except Exception as e:
            error_msg = f"å‘ç”Ÿé”™è¯¯: {str(e)}"
            st.error(error_msg)
            full_response = error_msg

    st.session_state.messages.append({"role": "assistant", "content": full_response})